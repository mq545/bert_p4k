{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "bert_magic.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR8Az0hNa0d4"
      },
      "source": [
        "## Classifying stuff with BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrF6Icpxa0d6",
        "outputId": "49d75a0d-b500-4b86-847d-43b218aa4481"
      },
      "source": [
        "# Tensorflow text is used for pre-processing\n",
        "!pip install -q tensorflow-text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.4MB 6.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2fL4SP_a0d8",
        "outputId": "996a2024-c395-4c5b-b4fe-fb7e74a3e07b"
      },
      "source": [
        "# We'll be using the ADAMW Optimizer from tf-models\n",
        "!pip install -q tf-models-official"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1MB 6.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 18.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 645kB 33.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 33.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 706kB 40.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 26.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 37.6MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 9.8MB/s \n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bp18s6Aa0d9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optmizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAL2KO4-b77k"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTOqNdy1cLOc"
      },
      "source": [
        "##We'll start with IMDB sentiment classifcation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOdqCMoQDRJL",
        "outputId": "f5d7e20e-a687-4b54-fa21-254e098e4bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
        "\n",
        "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
        "                                  untar=True, cache_dir='.',\n",
        "                                  cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "\n",
        "# remove unused folders to make it easier to load the data\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 5s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}